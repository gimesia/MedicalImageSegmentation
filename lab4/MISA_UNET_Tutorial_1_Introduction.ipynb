{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimesia/MedicalImageSegmentation/blob/main/MISA_UNET_Tutorial_1_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KxIplL-a3R9"
      },
      "source": [
        "\n",
        "**Tutorial # 1 - Introduction**\n",
        "\n",
        "Idea:\n",
        "\n",
        "1.   Train, validate, and test a segmentation network using keras\n",
        "2.   Test effect of skip connections (segnet -> unet)\n",
        "3.   Test effect of hyperparameters (batch size, patch size, strides, epochs, # kernels) -> nn-unet\n",
        "4.   Test effect of intensity standardisation\n",
        "5.   Deal with limited resources (hardware and images; patch-wise processing)\n",
        "6.   Get used to keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data: https://drive.google.com/drive/folders/1jKee0ry5sR_mvc2MvYmvBlOUF0fJ7suz?usp=drive_link"
      ],
      "metadata": {
        "id": "mTzlkMqBBtxy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIG5gOwa-Nb"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "spA2vbr3a0j7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqbV-aZEzEx"
      },
      "source": [
        "**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFJ_L_2i55hx",
        "outputId": "d7ee0c8d-1077-4276-f66b-b8737303766d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEFUu8FjYvaz"
      },
      "source": [
        "**Define parameters**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted\""
      ],
      "metadata": {
        "id": "U3703UzsEGn9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eerKWEtxUg_x"
      },
      "outputs": [],
      "source": [
        "# dataset parameters\n",
        "FNAME_PATTERN = f'{DATA_PATH}/OAS2_{0:04d}_MR1/OAS2_{0:04d}_MR1{1:}.nii.gz'\n",
        "IMAGE_SIZE = (128, 256, 256)\n",
        "\n",
        "# network parameters\n",
        "N_CLASSES = 4\n",
        "N_INPUT_CHANNELS = 1\n",
        "SCALING_FACTOR = 1\n",
        "PATCH_SIZE = (32, 32)\n",
        "PATCH_STRIDE = (32, 32)\n",
        "\n",
        "# training, validation, test parameters\n",
        "TRAINING_VOLUMES = [2, 7, 9, 10, 12, 13, 14, 16, 18]\n",
        "VALIDATION_VOLUMES = [21, 22, 26]\n",
        "TEST_VOLUMES = [20] # Difficult volumes: 4, 5, 8, 12, 17, 23, 28\n",
        "\n",
        "# data preparation parameters\n",
        "CONTENT_THRESHOLD = 0.3\n",
        "\n",
        "# training parameters\n",
        "N_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 10\n",
        "MODEL_FOLDER = 'checkpoint.keras'\n",
        "OPTIMISER = 'Adam'\n",
        "LOSS = 'categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kakQO5X9E0oR"
      },
      "source": [
        "**Define SegNet architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "flPaOqv0bZ3-"
      },
      "outputs": [],
      "source": [
        "def get_segnet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scaling_factor=SCALING_FACTOR):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding path\n",
        "    conv1 = layers.Conv2D(32*scaling_factor, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
        "    max1 = layers.MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(64*scaling_factor, (3, 3), padding=\"same\", activation='relu')(max1)\n",
        "    max2 = layers.MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(128*scaling_factor, (3, 3), padding=\"same\", activation='relu')(max2)\n",
        "    max3 = layers.MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    lat = layers.Conv2D(256*scaling_factor, (3, 3), padding=\"same\", activation='relu')(max3)\n",
        "\n",
        "    # Decoding path\n",
        "    # Add concatenation layers later on to create U-Nets\n",
        "    # example: cat = layers.Concatenate()([layer_1, layer_2])\n",
        "    # we could use additions instead of concatenations too\n",
        "    # example: cat = layers.Add()([layer_1, layer_2])\n",
        "\n",
        "    up1 = layers.UpSampling2D((2, 2))(lat)\n",
        "    conv4 = layers.Conv2D(128*scaling_factor, (3, 3), padding=\"same\", activation='relu')(up1)\n",
        "\n",
        "    up2 = layers.UpSampling2D((2, 2))(conv4)\n",
        "    conv5 = layers.Conv2D(64*scaling_factor, (3, 3), padding=\"same\", activation='relu')(up2)\n",
        "\n",
        "    up3 = layers.UpSampling2D((2, 2))(conv5)\n",
        "    conv6 = layers.Conv2D(32*scaling_factor, (3, 3), padding=\"same\", activation='relu')(up3)\n",
        "\n",
        "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv6)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1E1IglE-0w"
      },
      "source": [
        "**Load data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z-iEv0DKfkhD"
      },
      "outputs": [],
      "source": [
        "# Correcting FNAME_PATTERN\n",
        "FNAME_PATTERN = f'{DATA_PATH}/OAS2_{0:04d}_MR1/OAS2_{0:04d}_MR1{1}.nii.gz'\n",
        "\n",
        "# Correcting pattern lambda\n",
        "pattern = lambda x, y: f'{DATA_PATH}/OAS2_{x:04d}_MR1/OAS2_{x:04d}_MR1{y}.nii.gz'\n",
        "\n",
        "def load_data(volume_list, image_size=IMAGE_SIZE, fname_pattern=pattern):\n",
        "  n_volumes = len(volume_list)\n",
        "  T1_volumes = np.zeros((n_volumes, *image_size, 1))\n",
        "  labels = np.zeros((n_volumes, *image_size, 1))\n",
        "  for iFile, iID in enumerate(volume_list):\n",
        "    print(iID)\n",
        "    print(fname_pattern(iID, '_BrainExtractionBrain'))\n",
        "\n",
        "    img_data = nib.load(fname_pattern(iID, '_BrainExtractionBrain'))\n",
        "    T1_volumes[iFile, ..., 0] = np.transpose(img_data.get_fdata(), (2, 0, 1))\n",
        "\n",
        "    seg_data = nib.load(fname_pattern(iID, '_Segmentation'))\n",
        "    labels[iFile, ..., 0] = np.transpose(seg_data.get_fdata(), (2, 0, 1))\n",
        "\n",
        "  return (T1_volumes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern(21, \"__DEST\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rM5bBir3J1Pg",
        "outputId": "dcc215fb-4050-45e0-8202-fdbc557e8f64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0021_MR1/OAS2_0021_MR1__DEST.nii.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SCohHTitd5gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad775c3-7f29-4769-9ca7-988a9d2e6101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0002_MR1/OAS2_0002_MR1_BrainExtractionBrain.nii.gz\n",
            "7\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0007_MR1/OAS2_0007_MR1_BrainExtractionBrain.nii.gz\n",
            "9\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0009_MR1/OAS2_0009_MR1_BrainExtractionBrain.nii.gz\n",
            "10\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0010_MR1/OAS2_0010_MR1_BrainExtractionBrain.nii.gz\n",
            "12\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0012_MR1/OAS2_0012_MR1_BrainExtractionBrain.nii.gz\n",
            "13\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0013_MR1/OAS2_0013_MR1_BrainExtractionBrain.nii.gz\n",
            "14\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0014_MR1/OAS2_0014_MR1_BrainExtractionBrain.nii.gz\n",
            "16\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0016_MR1/OAS2_0016_MR1_BrainExtractionBrain.nii.gz\n",
            "18\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0018_MR1/OAS2_0018_MR1_BrainExtractionBrain.nii.gz\n",
            "21\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0021_MR1/OAS2_0021_MR1_BrainExtractionBrain.nii.gz\n",
            "22\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0022_MR1/OAS2_0022_MR1_BrainExtractionBrain.nii.gz\n",
            "26\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0026_MR1/OAS2_0026_MR1_BrainExtractionBrain.nii.gz\n",
            "20\n",
            "/content/drive/MyDrive/Colab Notebooks/OASIS_subset_sorted/OAS2_0020_MR1/OAS2_0020_MR1_BrainExtractionBrain.nii.gz\n"
          ]
        }
      ],
      "source": [
        "(training_volumes_T1, training_labels) = load_data(TRAINING_VOLUMES)\n",
        "(validation_volumes_T1, validation_labels) = load_data(VALIDATION_VOLUMES)\n",
        "(testing_volumes_T1, testing_labels) = load_data(TEST_VOLUMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpKWGJ-Fh1B_"
      },
      "source": [
        "**Pre-process data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_fKkb5ygPgvJ",
        "outputId": "6f1c0008-de48-49f5-eeb8-f20bfac3fb70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_volumes_T1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cf222748f973>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_volumes_T1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_volumes_T1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_volumes_T1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_volumes_T1' is not defined"
          ]
        }
      ],
      "source": [
        "plt.hist(training_volumes_T1[training_labels>0].flatten(), 100, label='training', alpha=0.5, density=True)\n",
        "plt.hist(validation_volumes_T1[validation_labels>0].flatten(), 100, label='validation', alpha=0.5, density=True)\n",
        "plt.hist(testing_volumes_T1[testing_labels>0].flatten(), 100, label='test', alpha=0.5, density=True)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XivmLjMOO4Nz"
      },
      "outputs": [],
      "source": [
        "# Space for trying intensity standardisation strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGim-slZQpXV"
      },
      "outputs": [],
      "source": [
        "plt.hist(training_volumes_T1[training_labels>0].flatten(), 100, label='training', alpha=0.5, density=True)\n",
        "plt.hist(validation_volumes_T1[validation_labels>0].flatten(), 100, label='validation', alpha=0.5, density=True)\n",
        "plt.hist(testing_volumes_T1[testing_labels>0].flatten(), 100, label='test', alpha=0.5, density=True)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDtzyW6FH7x"
      },
      "source": [
        "**Extract *useful* patches**\n",
        "\n",
        "This step is fundamental, we want to provide the network with useful information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4qyfiRaf7n2"
      },
      "outputs": [],
      "source": [
        "def extract_patches(x, patch_size, patch_stride) :\n",
        "  return tf.image.extract_patches(\n",
        "    x,\n",
        "    sizes=[1, *patch_size, 1],\n",
        "    strides=[1, *patch_stride, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding='SAME', name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsKNTm-Lf-sb"
      },
      "outputs": [],
      "source": [
        "def extract_useful_patches(\n",
        "    volumes, labels,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stride=PATCH_STRIDE,\n",
        "    threshold=CONTENT_THRESHOLD,\n",
        "    num_classes=N_CLASSES) :\n",
        "  volumes = volumes.reshape([-1, image_size[1], image_size[2], 1])\n",
        "  labels = labels.reshape([-1, image_size[1], image_size[2], 1])\n",
        "\n",
        "  vol_patches = extract_patches(volumes, patch_size, stride).numpy()\n",
        "  seg_patches = extract_patches(labels, patch_size, stride).numpy()\n",
        "\n",
        "  vol_patches = vol_patches.reshape([-1, *patch_size, 1])\n",
        "  seg_patches = seg_patches.reshape([-1, *patch_size, ])\n",
        "\n",
        "  foreground_mask = seg_patches != 0\n",
        "\n",
        "  useful_patches = foreground_mask.sum(axis=(1, 2)) > threshold * np.prod(patch_size)\n",
        "\n",
        "  vol_patches = vol_patches[useful_patches]\n",
        "  seg_patches = seg_patches[useful_patches]\n",
        "\n",
        "  seg_patches = tf.keras.utils.to_categorical(\n",
        "    seg_patches, num_classes=N_CLASSES)\n",
        "\n",
        "  return (vol_patches, seg_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcoEi426bfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "3e5267a0-ea4e-4407-a554-660195229bee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_volumes_T1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ce5608dd6e26>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract patches from training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtraining_patches_T1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_patches_seg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_useful_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_volumes_T1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# extract patches from validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_patches_T1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_patches_seg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_useful_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_volumes_T1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_volumes_T1' is not defined"
          ]
        }
      ],
      "source": [
        "# extract patches from training set\n",
        "(training_patches_T1, training_patches_seg) = extract_useful_patches(training_volumes_T1, training_labels)\n",
        "\n",
        "# extract patches from validation set\n",
        "(validation_patches_T1, validation_patches_seg) = extract_useful_patches(validation_volumes_T1, validation_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJYxf3HiFopr"
      },
      "source": [
        "**Instantiate SegNet model and train it**\n",
        "\n",
        "*   When/how do we stop training?\n",
        "*   Should we use validation split in keras?\n",
        "*   How large should the batch size be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7M1Az8uI1BE"
      },
      "outputs": [],
      "source": [
        "segnet = get_segnet()\n",
        "segnet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "h = segnet.fit(\n",
        "    x=training_patches_T1,\n",
        "    y=training_patches_seg,\n",
        "    validation_data=(validation_patches_T1, validation_patches_seg),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=N_EPOCHS,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjIfjtiaKYr1"
      },
      "source": [
        "We could stop training when validation loss increases substantially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bspE_hozI9zN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a0972029-7e35-4cf8-e52e-39a626ed7ed2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'h' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e2eadb427e70>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(N_EPOCHS), h.history['loss'], label='loss')\n",
        "plt.plot(range(N_EPOCHS), h.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkBp70aWGB_b"
      },
      "source": [
        "Using callbacks to stop training and avoid overfitting\n",
        "\n",
        "\n",
        "*   Early stopping with a certain patience\n",
        "*   Save (and load!) best model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "j7H9hIPwEwJZ",
        "outputId": "56c53779-8c36-4be3-9f4a-a1dd885248be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_patches_T1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-338c93a43cde>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msegnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPTIMISER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOSS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m h = segnet.fit(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_patches_T1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_patches_seg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_patches_T1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_patches_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_patches_T1' is not defined"
          ]
        }
      ],
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FOLDER, save_best_only=True)\n",
        "]\n",
        "\n",
        "segnet = get_segnet()\n",
        "segnet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "h = segnet.fit(\n",
        "    x=training_patches_T1,\n",
        "    y=training_patches_seg,\n",
        "    validation_data=(validation_patches_T1, validation_patches_seg),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=N_EPOCHS,\n",
        "    callbacks=my_callbacks,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGcfrZCMjz4M"
      },
      "source": [
        "**Load best model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fNsYJL7KnON"
      },
      "outputs": [],
      "source": [
        "segnet = get_segnet(\n",
        "    img_size=(IMAGE_SIZE[1], IMAGE_SIZE[2]),\n",
        "    n_classes=N_CLASSES,\n",
        "    n_input_channels=N_INPUT_CHANNELS)\n",
        "\n",
        "segnet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "segnet.load_weights(MODEL_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwPzN0jrSm8r"
      },
      "source": [
        "**Prepare test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuRHLNYGkDFp"
      },
      "outputs": [],
      "source": [
        "testing_volumes_T1_processed = testing_volumes_T1.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "testing_labels_processed = testing_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbJZUQyLqF4p"
      },
      "source": [
        "**Predict labels for test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItoY31x0K3r8"
      },
      "outputs": [],
      "source": [
        "prediction = segnet.predict(x=testing_volumes_T1_processed)\n",
        "\n",
        "prediction = np.argmax(prediction, axis=3)\n",
        "\n",
        "plt.imshow(prediction[:, :, 128])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT2ZICiRH4BG"
      },
      "outputs": [],
      "source": [
        "!pip install medpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FraQquLu-Lv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from medpy.metric.binary import dc, hd, ravd\n",
        "\n",
        "def compute_dice(prediction, reference) :\n",
        "  for c in np.unique(reference) :\n",
        "    dsc_val = dc(prediction == c, reference==c)\n",
        "    print(f'Dice coefficient class {c} equal to {dsc_val : .2f}')\n",
        "\n",
        "def compute_hd(prediction, reference, voxel_spacing) :\n",
        "  for c in np.unique(prediction) :\n",
        "    hd_val = hd(prediction == c, reference==c, voxelspacing=voxel_spacing, connectivity=1)\n",
        "    print(f'Hausdorff distance class {c} equal to {hd_val : .2f}')\n",
        "\n",
        "def compute_ravd(prediction, reference) :\n",
        "  for c in np.unique(prediction) :\n",
        "    ravd_val = ravd(prediction == c, reference==c)\n",
        "    print(f'RAVD coefficient class {c} equal to {ravd_val : .2f}')\n",
        "\n",
        "compute_dice(prediction, testing_labels_processed) # the higher -> the better\n",
        "compute_hd(prediction, testing_labels_processed, [1, 1, 1]) # the lower -> the better\n",
        "compute_ravd(prediction, testing_labels_processed) # the closer to zero -> the better"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AML_GrHjEumI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}